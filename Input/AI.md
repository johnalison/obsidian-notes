# Some AI thoughts



Artificial Intelligence
Are we creating our own gods?

- With trying to develop AI we are trying — racing— to create actual, no-bullshit Gods and almost no one is talking about it.  

  > Its also not at all clear we will have much control over what kind
    of gods we create, and the consensus among experts is that it is
    something we are likely to do in the next 50-100 years and only
    get one shot at. Seems likely to lead to immortality or
    extinction.

 > But, How much do people spend talking about the fact that soon they
   will spend the rest of eternity not existing ?

- How much smarter than humans can something get ? 

   > With narrow intelligence its clear there is a lot of
     room. General Intelligence its not obvious, eg: could asymptotic
     with humans being close to the max.  However even if “only” as
     smart as humans, AI will still be 10^6 times faster and have vast
     improvements in a wide range of narrow intelligence.

- AI is a tragedy of the commons: 

   > Globally want to take our time with AI (huge potential
     downside). Personally, (without radical longevity) I want to
     accelerate AU b/c the downside of not getting AI and dying is
     (almost certainly) the same as getting bad AI.

- Like Elons response: Try to become the AI. Requires increasing bandwidth via neural-link.

- We tend to use logic and reason after the fact to justify our decisions to ourselves and others. (What does this say about "interpretable AI"?)


- Great podcase with Elizer Yudokowsky
  - Crazy to think that the AI can be boxed in.
  - Fire alarms are to get people to act as if its an emergency (not there to detect smoke) "There are no fire alarms for general AI"
  - The big story about alpha-go was not alpha-go beating the best go player, it was about alpha-0 beating the work class AI engineers.